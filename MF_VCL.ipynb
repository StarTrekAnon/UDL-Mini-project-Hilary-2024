{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "GjYMzWneLGDq",
        "outputId": "962ca69e-cfb2-467f-8d34-8921a0d98804"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Using device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "# @title Imports & Data\n",
        "import itertools\n",
        "import math\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "from torch.distributions import Normal\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "print(torch.cuda.is_available())\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "def load_mnist(digits = None, conv = False):\n",
        "    # Get MNIST test data\n",
        "    #X_train, Y_train, X_test, Y_test = data_mnist()\n",
        "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "    X_train = X_train.reshape(60000, 784)\n",
        "    X_test = X_test.reshape(10000, 784)\n",
        "    X_train = X_train.astype('float32')\n",
        "    X_test = X_test.astype('float32')\n",
        "    X_train /= 255\n",
        "    X_test /= 255\n",
        "\n",
        "    # convert class vectors to binary class matrices\n",
        "    Y_train = to_categorical(y_train, 10)\n",
        "    Y_test = to_categorical(y_test, 10)\n",
        "\n",
        "    # collect the corresponding digits\n",
        "    if digits is not None:\n",
        "        ind_train = []\n",
        "        ind_test = []\n",
        "        for i in digits:\n",
        "            ind_train = ind_train + list(np.where(Y_train[:, i] == 1)[0])\n",
        "            ind_test = ind_test + list(np.where(Y_test[:, i] == 1)[0])\n",
        "        X_train = X_train[ind_train]; Y_train = Y_train[ind_train]\n",
        "        X_test = X_test[ind_test]; Y_test = Y_test[ind_test]\n",
        "\n",
        "    if conv:\n",
        "        X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
        "        X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
        "\n",
        "    return X_train, X_test, Y_train, Y_test\n",
        "\n",
        "def single_digit_loader(X, label, b_size=10):\n",
        "    X.reshape(-1,dimX)\n",
        "    N = X.shape[0]\n",
        "    for i in range(N//b_size):\n",
        "        yield (torch.from_numpy(X[i*b_size:(i+1)*b_size,:]), torch.from_numpy(np.ones(b_size,dtype=int)))\n",
        "    if (N/b_size != 0.0):\n",
        "        end = list(range((N//b_size)*b_size,X.shape[0]))\n",
        "        n_missing = b_size - len(end)\n",
        "        last_batch_ind = end + list(range(n_missing))\n",
        "        yield (torch.from_numpy(X[last_batch_ind,:]), torch.from_numpy(np.ones(b_size,dtype=int)*label))\n",
        "\n",
        "\n",
        "\n",
        "def create_mnist_single_digit_loaders(b_size=10, train_data=True):\n",
        "    loaders = []\n",
        "    for i in range(10):\n",
        "        X_train, X_test, Y_train, Y_test = load_mnist(digits = [i])\n",
        "        if degenerate_dataset:\n",
        "            X_train = X_train[0,:].reshape(1,-1).repeat(X_train.shape[0], axis=0)\n",
        "        if train_data:\n",
        "            N_train = int(X_train.shape[0] * 0.9) if scale_down_090 else X_train.shape[0]\n",
        "            X_train = X_train[:N_train]\n",
        "            loaders.append(BatchWrapper(X_train,i,b_size))\n",
        "        else:\n",
        "            loaders.append(BatchWrapper(X_test, i, b_size))\n",
        "    return loaders"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Evaluation Function (Test LL)\n",
        "\n",
        "def IS_estimate(x, task_model, K):\n",
        "    x = x.view(-1, 28 ** 2)\n",
        "    x_rep = x.repeat([K, 1]).to(device=device)\n",
        "    assert(x_rep.size()[0] < 6000)\n",
        "\n",
        "    N = x.size()[0]\n",
        "    Zs_params = task_model.enc(x_rep)\n",
        "    mu_qz, log_sig_qz = Zs_to_mu_sig(Zs_params)\n",
        "    z = task_model.sampler(Zs_params)\n",
        "    mu_x = task_model.dec_shared(task_model.dec_head(z))\n",
        "    logp = log_bernoulli(x_rep, mu_x)\n",
        "\n",
        "    log_prior = log_gaussian_prob(z)\n",
        "    logq = log_gaussian_prob(z, mu_qz, log_sig_qz)\n",
        "    kl_z = logq - log_prior\n",
        "\n",
        "    bound = torch.reshape(logp - kl_z, (K, N))\n",
        "    bound_max = torch.max(bound, 0)[0]\n",
        "    bound -= bound_max\n",
        "    log_norm = torch.log(torch.clamp(torch.mean(torch.exp(bound), 0), 1e-9, np.inf))\n",
        "\n",
        "    test_ll = log_norm + bound_max\n",
        "    test_ll_mean = torch.mean(test_ll).item()\n",
        "    test_ll_var = torch.mean((test_ll - test_ll_mean) ** 2).item()\n",
        "\n",
        "    return test_ll_mean, test_ll_var\n",
        "\n",
        "\n",
        "class Evaluation:\n",
        "\n",
        "    def __init__(self, should_print=True, K=100):\n",
        "        self.should_print = should_print\n",
        "        self.K = K\n",
        "\n",
        "    def __call__(self, task_id, task_model, loader):\n",
        "        N = 0\n",
        "        bound_tot = 0.0\n",
        "        bound_var = 0.0\n",
        "        begin = time.time()\n",
        "        batches = len(loader)\n",
        "        for j in range(len(loader)):\n",
        "            inputs, labels = loader[j]\n",
        "            N += len(inputs)\n",
        "            logp_mean, logp_var = IS_estimate(inputs, task_model, self.K)\n",
        "            bound_tot += logp_mean / batches\n",
        "            bound_var += logp_var / batches\n",
        "        end = time.time()\n",
        "        if self.should_print:\n",
        "            print(\"task %d test_ll=%.2f, ste=%.2f, time=%.2f\" \\\n",
        "                  % (task_id, bound_tot, np.sqrt(bound_var / N), end - begin))\n",
        "        return (bound_tot, np.sqrt(bound_var / N))\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "l5VhNkYgLnQY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Sampling Function\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def KL_div_gaussian(mu_p, log_sig_p, mu_q, log_sig_q):\n",
        "    # compute KL[p||q]\n",
        "    precision_q = torch.exp(-2 * log_sig_q)\n",
        "    kl = 0.5 * (mu_p - mu_q) ** 2 * precision_q - 0.5\n",
        "    kl += log_sig_q - log_sig_p\n",
        "    kl += 0.5 * torch.exp(2 * log_sig_p - 2 * log_sig_q)\n",
        "    return torch.sum(kl, dim=list(range(1, len(kl.shape))))\n",
        "\n",
        "def KL_div_gaussian_from_standard_normal(mu_q, log_sig_q):\n",
        "    # 0,0 corresponds to N(0,1) due to the log_sig representation, works for multidim normal as well.\n",
        "    return KL_div_gaussian(mu_q, log_sig_q, torch.zeros(1, device=device), torch.zeros(1, device=device))\n",
        "\n",
        "def Zs_to_mu_sig(Zs_params):\n",
        "    dimZ = Zs_params.shape[1] // 2  # 1st is batch size 2nd is 2*dimZ\n",
        "    mu_qz = Zs_params[:, :dimZ]\n",
        "    log_sig_qz = Zs_params[:, dimZ:]\n",
        "    return mu_qz, log_sig_qz\n",
        "\n",
        "forced_interval = (1e-9, 1.0)\n",
        "\n",
        "def log_bernoulli(X, Mu_Reconstructed_X):\n",
        "    \"\"\"\n",
        "    Mu_Reconstructed_X is the output of the decoder. We accept fractions, and project them to the interval 'forced_interval' for numerical stability\n",
        "    \"\"\"\n",
        "    logprob = X * torch.log(torch.clamp(Mu_Reconstructed_X, *forced_interval)) + (1 - X) * torch.log(torch.clamp((1.0 - Mu_Reconstructed_X), *forced_interval))\n",
        "\n",
        "    return torch.sum(logprob.view(logprob.size()[0], -1), dim=1)  # sum all but first dim\n",
        "\n",
        "def log_gaussian_prob(x, mu=torch.zeros(1, device=device), log_sig=torch.zeros(1, device=device)):\n",
        "    logprob = -(0.5 * np.log(2 * np.pi) + log_sig) \\\n",
        "              - 0.5 * ((x - mu) / torch.exp(log_sig)) ** 2\n",
        "    return torch.sum(logprob.view(logprob.size()[0], -1), dim=1)  # sum all but first dim\n",
        "\n",
        "def log_P_y_GIVEN_x(Xs, enc, sample_and_decode, NumLogPSamples=100):\n",
        "    \"\"\"\n",
        "    Returns logP(Y|X), KL(Z||Normal(0,1))\n",
        "    \"\"\"\n",
        "    Zs_params = enc(Xs)\n",
        "    mu_qz, log_sig_qz = Zs_to_mu_sig(Zs_params)\n",
        "    kl_z = KL_div_gaussian_from_standard_normal(mu_qz, log_sig_qz)\n",
        "    logp = 0.0\n",
        "    for _ in range(NumLogPSamples):\n",
        "        # The Zs_params are the deterministic result of enc(Xs) so we don't recalculate them\n",
        "        Mu_Ys = sample_and_decode(Zs_params)\n",
        "        logp += log_bernoulli(Xs, Mu_Ys) / NumLogPSamples\n",
        "    return logp, kl_z\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "z97UKoYMLtFj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Encoder\n",
        "class mlp_layer(nn.Module):\n",
        "    def __init__(self, d_in, d_out, activation):\n",
        "        \"\"\"\n",
        "        Activation is a function (eg. torch.nn.functional.sigmoid/relu)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.mu = nn.Linear(d_in, d_out).to(device=device)\n",
        "        with torch.no_grad():\n",
        "            self._init_weights(d_in, d_out)\n",
        "        self.activation = activation\n",
        "\n",
        "    def forward(self, x):\n",
        "        if (weight_print):\n",
        "            print(\"Weights of ENC \", self.mu.weight)\n",
        "            print(\"bias of ENC \", self.mu.bias)\n",
        "        return self.activation(self.mu(x))\n",
        "\n",
        "    def _init_weights(self, input_size, output_size, constant=1.0):\n",
        "        scale = constant * np.sqrt(6.0 / (input_size + output_size))\n",
        "        assert (output_size > 0)\n",
        "        nn.init.uniform_(self.mu.weight, -scale, scale)\n",
        "        nn.init.zeros_(self.mu.bias)\n",
        "\n",
        "    @property\n",
        "    def d_out(self):\n",
        "        return self.mu.weight.shape[0]\n",
        "\n",
        "    @property\n",
        "    def d_in(self):\n",
        "        return self.mu.weight.shape[1]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1SayqeOfLzam"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Bayesian Decoder\n",
        "class bayesian_mlp_layer(mlp_layer):\n",
        "    def __init__(self, d_in, d_out, activation):\n",
        "        \"\"\"\n",
        "        Activation is a function (eg. torch.nn.functional.sigmoid/relu)\n",
        "        \"\"\"\n",
        "        super().__init__(d_in, d_out, activation)\n",
        "        self.log_sigma = nn.Linear(d_in, d_out).to(device=device)\n",
        "        with torch.no_grad():\n",
        "            self._init_log_sigma()\n",
        "        # mu is initialized the same as non-Bayesian mlp\n",
        "\n",
        "        self.w_standard_normal_sampler = Normal(torch.zeros(self.mu.weight.shape, device=device), torch.ones(self.mu.weight.shape, device=device))\n",
        "        self.b_standard_normal_sampler = Normal(torch.zeros(self.mu.bias.shape, device=device), torch.ones(self.mu.bias.shape, device=device))\n",
        "\n",
        "        self.sampling = True\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        if (weight_print):\n",
        "            print(\"Weights of mu DEC \", self.mu.weight)\n",
        "            print(\"bias of mu DEC \", self.mu.bias)\n",
        "            print(\"Weights of log_sig DEC \", self.log_sigma.weight)\n",
        "            print(\"bias of log_sig DEC \", self.log_sigma.bias)\n",
        "\n",
        "        if self.sampling:\n",
        "            sampled_W = (self.mu.weight + torch.randn_like(self.mu.weight) * torch.exp(self.log_sigma.weight))\n",
        "            sampled_b = (self.mu.bias + torch.randn_like(self.mu.bias) * torch.exp(self.log_sigma.bias))\n",
        "            return self.activation(torch.einsum('ij,bj->bi',[sampled_W, x]) + sampled_b)\n",
        "        else:\n",
        "            return super().forward(x)\n",
        "\n",
        "    def _init_log_sigma(self):\n",
        "        nn.init.constant_(self.log_sigma.weight, -6.0)\n",
        "        nn.init.constant_(self.log_sigma.bias, -6.0)\n",
        "\n",
        "    def get_posterior(self):\n",
        "        return [(self.mu.weight, self.log_sigma.weight), (self.mu.bias, self.log_sigma.bias)]\n",
        "\n",
        "class NormalSamplingLayer(nn.Module):\n",
        "    def __init__(self, d_out):\n",
        "        super().__init__()\n",
        "        self.d_out = d_out\n",
        "\n",
        "    def forward(self, mu_log_sigma_vec):\n",
        "        mu = mu_log_sigma_vec[:, :self.d_out]\n",
        "        return mu + torch.randn_like(mu) * torch.exp(mu_log_sigma_vec[:, self.d_out:])\n",
        "\n",
        "class SharedDecoder(nn.Module):\n",
        "    def __init__(self, dims, activations):\n",
        "        super().__init__()\n",
        "        # Not sure if device does anything\n",
        "        self.net = nn.Sequential(*[bayesian_mlp_layer(dims[i], dims[i + 1], activations[i]) \\\n",
        "                                   for i in range(len(activations))])\n",
        "        self._init_prior()\n",
        "\n",
        "    def forward(self, Xs):\n",
        "        return self.net(Xs)\n",
        "\n",
        "    def _get_posterior(self):\n",
        "        return list(itertools.chain(*list(map(lambda f: f.get_posterior(), self.net.children()))))\n",
        "\n",
        "    def _init_prior(self):\n",
        "        \"\"\"\n",
        "        Initialize a constant tensor that corresponds to a prior distribution over all the weights\n",
        "        which is standard normal\n",
        "        \"\"\"\n",
        "        self.prior = [(torch.zeros(mu.shape, device=device), torch.zeros(log_sig.shape, device=device)) for mu, log_sig in self._get_posterior()]\n",
        "        for mu, log_sig in self.prior:\n",
        "            mu.requires_grad = False\n",
        "            log_sig.requires_grad = False\n",
        "\n",
        "    def update_prior(self):\n",
        "        \"\"\"\n",
        "        Copy the current posterior to a constant tensor, which will be used as prior for the next task\n",
        "        \"\"\"\n",
        "        posterior = self._get_posterior()\n",
        "        self.prior = [(mu.clone().detach(), log_sig.clone().detach()) for mu, log_sig in posterior]\n",
        "        #update the new posterior's log_sig to -6\n",
        "        with torch.no_grad():\n",
        "            for mu_sig in posterior:\n",
        "                mu_sig[1].fill_(-6.0)\n",
        "\n",
        "\n",
        "    def KL_from_prior(self):\n",
        "        params = [(*post, *prior) for (post, prior) in zip(self._get_posterior(), self.prior)]\n",
        "        KL = torch.zeros(1, device=device).squeeze() #don't know how to generate a zero scalar\n",
        "        for param in params:\n",
        "            unsqueezed_param = list(map(lambda x: x.unsqueeze(0), param))\n",
        "            tmp = KL_div_gaussian(*unsqueezed_param)\n",
        "            KL += tmp.squeeze()\n",
        "\n",
        "        return KL\n",
        "\n",
        "    @property\n",
        "    def d_in(self):\n",
        "        return self.net.d_in\n",
        "\n",
        "    @property\n",
        "    def d_out(self):\n",
        "        return self.net.d_out"
      ],
      "metadata": {
        "cellView": "form",
        "id": "pr2f1-aTL0ag"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Experiment 1 (Mean-Field Bayesian VAE)\n",
        "\n",
        "\n",
        "DATASET = 'mnist'\n",
        "\n",
        "Train = True\n",
        "max_task = 10\n",
        "\n",
        "weight_print = False\n",
        "data_print = False\n",
        "loss_print = False\n",
        "\n",
        "scale_down_090 = True\n",
        "degenerate_dataset = False\n",
        "\n",
        "dimX = 28 * 28\n",
        "dimH = 500\n",
        "dimZ = 50\n",
        "batch_size = 50\n",
        "n_epochs = 400\n",
        "\n",
        "# Encoder\n",
        "enc_dims = [dimX, dimH, dimZ * 2, dimH, dimZ * 2]\n",
        "enc_activations = [F.relu_, lambda x: x, F.relu_, lambda x: x]\n",
        "\n",
        "# Sample here\n",
        "\n",
        "# Private decoder (Head)\n",
        "dec_head_dims = [dimZ, dimH, dimZ]\n",
        "dec_head_activations = [F.relu_, lambda x: x]\n",
        "\n",
        "# Sample here\n",
        "\n",
        "# Shared decoder\n",
        "dec_shared_dims = [dimZ, dimH, dimX]\n",
        "dec_shared_activations = [F.relu_, torch.sigmoid]\n",
        "\n",
        "# BayesianVAE\n",
        "class TaskModel(nn.Module):\n",
        "    def __init__(self, enc_dims_activations, dec_head_dims_activations, dec_shared, learning_rate=1e-4):\n",
        "        super().__init__()\n",
        "        # Define Encoder\n",
        "        my_enc_dims, my_enc_activations = enc_dims_activations\n",
        "\n",
        "        self.enc = nn.Sequential(*[mlp_layer(my_enc_dims[i], my_enc_dims[i + 1], my_enc_activations[i])\n",
        "                                   for i in range(len(my_enc_activations))])\n",
        "\n",
        "        # Define private decoder (head)\n",
        "        my_dec_head_dims, my_dec_head_activations = dec_head_dims_activations\n",
        "\n",
        "        self.dec_head = nn.Sequential(\n",
        "            *[bayesian_mlp_layer(my_dec_head_dims[i], my_dec_head_dims[i + 1], my_dec_head_activations[i])\n",
        "              for i in range(len(my_dec_head_activations))])\n",
        "\n",
        "        # Define shared decoder\n",
        "        self.dec_shared = dec_shared\n",
        "        self.printer = PrintLayer()\n",
        "\n",
        "        # Combine components\n",
        "        self.sampler = NormalSamplingLayer(my_dec_head_dims[0])\n",
        "        self.sample_and_decode = nn.Sequential(*[self.sampler, self.dec_head, self.dec_shared])\n",
        "        self.decode = nn.Sequential(*[self.dec_head, self.dec_shared])\n",
        "\n",
        "        # update just before training\n",
        "        self.DatasetSize = None\n",
        "\n",
        "        # Guards from retraining- train only once\n",
        "        self.TrainGuard = True\n",
        "\n",
        "        self.optimizer = self._create_optimizer(learning_rate)\n",
        "\n",
        "    def set_sampling(self, sampling):\n",
        "        for model in self.dec_head.children():\n",
        "            model.sampling = sampling\n",
        "        for model in self.dec_shared.net.children():\n",
        "            model.sampling = sampling\n",
        "\n",
        "    def save_model(self, path):\n",
        "        torch.save(self.state_dict(), path)\n",
        "\n",
        "    @classmethod\n",
        "    def load_model(cls, path, uninitialized_instance):\n",
        "        uninitialized_instance.load_state_dict(torch.load(path,map_location=device))\n",
        "        uninitialized_instance.eval()\n",
        "        return uninitialized_instance\n",
        "\n",
        "    def forward(self, Xs):\n",
        "        logp, kl_z = log_P_y_GIVEN_x(Xs, self.enc, self.sample_and_decode)  #POINTER\n",
        "        kl_shared_dec_Qt_2_PREV_Qt = self.dec_shared.KL_from_prior()\n",
        "\n",
        "        # We ignore the kl(private dec || Normal(0,1) ) like the authors did\n",
        "        logp_mean = torch.mean(logp)\n",
        "        kl_z_mean = torch.mean(kl_z)\n",
        "        kl_Qt_normalized = (kl_shared_dec_Qt_2_PREV_Qt / self.DatasetSize)\n",
        "        ELBO = logp_mean - kl_z_mean - kl_Qt_normalized\n",
        "        #if loss_print:\n",
        "        #    print(\"Log_like\", \"\\tKL Z\",\"\\tKL Qt vs prev Qt\")\n",
        "        #    print('%.2f\\t%.2f\\t%.2f' % (logp_mean, kl_z_mean, kl_Qt_normalized))\n",
        "\n",
        "        return -ELBO\n",
        "\n",
        "    def _create_optimizer(self, learning_rate):\n",
        "        return torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "    def _update_prior(self):\n",
        "        self.dec_shared.update_prior()\n",
        "        # no other priors should be updated. they are trained once.\n",
        "        return\n",
        "\n",
        "    def train_model(self, n_epochs, task_trainloader, DatasetSize):\n",
        "        # We don't intend a TaskModel to be trained more than once\n",
        "        assert (self.TrainGuard)\n",
        "        self.TrainGuard = False\n",
        "\n",
        "        self.DatasetSize = DatasetSize\n",
        "\n",
        "        # loop over the dataset multiple times\n",
        "        for epoch in range(n_epochs):\n",
        "            print(\"starting epoch \" + str(epoch))\n",
        "            task_trainloader.shuffle()\n",
        "            running_loss = 0.0\n",
        "            for i in range(len(task_trainloader)):\n",
        "                global loss_print\n",
        "                loss_print = True #(i % 20 == 19)\n",
        "                # get the inputs\n",
        "                inputs, labels = task_trainloader[i]\n",
        "                #Migrate to device (gpu if possible)\n",
        "                inputs = inputs.to(device=device)\n",
        "                # step\n",
        "                self.optimizer.zero_grad()\n",
        "                # loss = self(inputs.view(-1, self.enc.d_in))\n",
        "                loss = self(inputs.view(-1, 28 ** 2))\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                # print statistics\n",
        "                running_loss += loss.item()  # ?\n",
        "                if i % 20 == 19:  # print every 2000 mini-batches\n",
        "                    print('[%d, %5d] loss: %.3f' %\n",
        "                          (epoch + 1, i + 1, running_loss/20))\n",
        "                    running_loss = 0.0\n",
        "        # This will set the prior to the current posterior, before we start to change it during training\n",
        "        self._update_prior()\n",
        "        self.DatasetSize = None\n",
        "\n",
        "\n",
        "class PrintLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PrintLayer, self).__init__()\n",
        "        self.count = 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        if (data_print):\n",
        "            self.count += 1\n",
        "            print(\"PRINTER LAYER\")\n",
        "            print(self.count)\n",
        "            print(x[0].shape)\n",
        "            print(x[0])\n",
        "        return x\n",
        "\n",
        "\n",
        "# In[135]:\n",
        "\n",
        "class BatchWrapper:\n",
        "    def __init__(self, X, label, b_size):\n",
        "        self.X = X.reshape(-1, dimX).astype('float32')\n",
        "        self.N = X.shape[0]\n",
        "        self.label = label\n",
        "        self.b_size = b_size\n",
        "\n",
        "    def shuffle(self):\n",
        "        np.random.shuffle(self.X)\n",
        "\n",
        "    def flat_size(self):\n",
        "        return self.X.shape[0]\n",
        "\n",
        "    def __len__(self):\n",
        "        if (self.N % self.b_size ==0):\n",
        "            return self.N//self.b_size\n",
        "        else:\n",
        "            return (self.N//self.b_size) +1\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        assert(i>=0 and i < len(self))\n",
        "        if i == len(self) - 1:\n",
        "            end = list(range((self.N // self.b_size) * self.b_size, self.N))\n",
        "            n_missing = self.b_size - len(end)\n",
        "            last_batch_ind = end + list(range(n_missing))\n",
        "            return (torch.from_numpy(self.X[last_batch_ind, :]), torch.from_numpy(np.ones(self.b_size, dtype=int) * self.label))\n",
        "        else:\n",
        "            return (torch.from_numpy(self.X[i*self.b_size:(i+1)*self.b_size,:]), torch.from_numpy(np.ones(self.b_size,dtype=int)))\n",
        "\n",
        "def path(after, i):\n",
        "    return './BestParamsBackup/after_task_' + str(after) + '_params_for_task_' + str(i) + '.pt'\n",
        "\n",
        "def load_models(after):\n",
        "    dec_shared = SharedDecoder(dec_shared_dims, dec_shared_activations)\n",
        "    models = []\n",
        "    for task_id in range(after+1): #range is 0 based\n",
        "        # Not sure if device does anything\n",
        "        task_model = TaskModel((enc_dims, enc_activations), (dec_head_dims, dec_head_activations), dec_shared)\n",
        "        models.append(TaskModel.load_model(path(after,task_id), task_model))\n",
        "    return models\n",
        "\n",
        "def generate_pictures(task_models, n_pics=100):\n",
        "    with torch.no_grad():\n",
        "        for task_id, task_model in enumerate(task_models):\n",
        "            task_model.set_sampling(False)\n",
        "            pics = task_model.sample_and_decode(torch.zeros(n_pics, dimZ * 2, device=device))\n",
        "            task_model.set_sampling(True)\n",
        "            pics = pics.cpu()\n",
        "            plot_images(pics, (28, 28), './figs/', 'after_task_'+str(len(task_models))+'_task_'+str(task_id))\n",
        "\n",
        "def generate_all_picture_row(task_models, n_pics=10):\n",
        "    row = np.zeros([n_pics, dimX])\n",
        "    with torch.no_grad():\n",
        "        for task_id, task_model in enumerate(task_models):\n",
        "            task_model.set_sampling(False)\n",
        "            pic = task_model.sample_and_decode(torch.zeros(1, dimZ * 2, device=device))\n",
        "            task_model.set_sampling(True)\n",
        "            pic = pic.cpu()\n",
        "            row[task_id] = pic\n",
        "    return row\n",
        "\n",
        "def main():\n",
        "    result_path = \"./results/MNIST_Torch_cla_results\"\n",
        "    dec_shared = SharedDecoder(dec_shared_dims, dec_shared_activations)\n",
        "\n",
        "    task_loaders = zip(create_mnist_single_digit_loaders(batch_size), create_mnist_single_digit_loaders(batch_size, train_data=False))\n",
        "\n",
        "    test_loaders = []\n",
        "    models = []\n",
        "\n",
        "    evaluators = [Evaluation()] #classifier is loaded. assumes already trained\n",
        "\n",
        "    results = np.zeros((len(evaluators), max_task, max_task))\n",
        "\n",
        "    # A task corresponds to a digit\n",
        "    for task_id,  (train_loader, test_loader) in enumerate(task_loaders):\n",
        "        if task_id>max_task:\n",
        "            break\n",
        "        print(\"starting task \" + str(task_id))\n",
        "        if (Train):\n",
        "            task_model = TaskModel((enc_dims, enc_activations), (dec_head_dims, dec_head_activations), dec_shared)\n",
        "            models.append(task_model)\n",
        "            task_model.train_model(n_epochs, train_loader, train_loader.flat_size())\n",
        "        else:\n",
        "            models = load_models(task_id)\n",
        "            task_model = models[-1]\n",
        "        test_loaders.append(test_loader)\n",
        "        #Disable gradient calculation during evaluation\n",
        "        with torch.no_grad():\n",
        "            \"\"\"\n",
        "            if (Train):\n",
        "                for i, model in enumerate(models):\n",
        "                    model.save_model(path(task_id, i))\n",
        "            \"\"\"\n",
        "            #generate_pictures(models)\n",
        "            #tmp = generate_all_picture_row(models)\n",
        "            #if task_id == 0:\n",
        "            #    all_pic = tmp\n",
        "            #else:\n",
        "            #    all_pic = np.concatenate([all_pic, tmp], 0)\n",
        "\n",
        "\n",
        "            for test_task_id, loader in enumerate(test_loaders):\n",
        "                for eval_idx, evaluator in enumerate(evaluators):\n",
        "                    results[eval_idx,task_id, test_task_id], _ = evaluator(test_task_id, models[test_task_id], loader)\n",
        "\n",
        "        print(results)\n",
        "\n",
        "    #np.save(result_path, results)\n",
        "    #with torch.no_grad():\n",
        "    #    plot_images(all_pic, (28, 28), './figs/', 'all_models_after_each_tasks')\n",
        "\n",
        "\n",
        "# In[137]:\n",
        "\n",
        "main()\n",
        "\n",
        "# In[ ]:"
      ],
      "metadata": {
        "cellView": "form",
        "id": "eFGlgfq_L9zh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}